<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ruisen Tu</title>

    <link rel="stylesheet" href="src/css/main.css">
    <!-- <link rel="icon" type="image/png" href="src/assets/potato.jpg"> -->
    <link rel="icon" type="image/png" href="src/assets/manga_avatar.jpeg">
    <!-- font-awesome -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" rel="stylesheet">
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Rubik&display=swap&family=Kanit:wght@300&family=Mina&display=swap" rel="stylesheet">
</head>
<body>
    <nav>
        <span class="name">Ruisen Tu</span>
        <div class="nav-right">
            <a href="" id="a-home">Home</a>
            <a href="" id="a-publications">Publications</a>
            <a href="" id="a-research">Research</a>
        </div>
    </nav>
    
    <div class="content">
        <div id="intro">
            <!-- <img src="src/assets/potato.jpg" alt="My selfie that only wise people can see"> -->
            <img src="src/assets/avatar.jpeg" alt="My selfie that only wise people can see">
            <div class="info">
                <h1>Ruisen Tu</h1>
                <p>Incoming CSE Master's Student @ UCSD</p>
                <div><a href="mailto:ruisent2@illinois.edu" title="email"><i class="fa-solid fa-envelope fa-2xl"></i></a></div>
                <!-- <div><a href="./src/assets/CV.pdf" title="CV" target="_blank"><i class="fa-solid fa-file fa-2xl"></i></a></div> -->
                <div><a href="https://drive.google.com/file/d/1hiemRxemuiROjq4fD-niNQ1BmMIqiqck/view?usp=sharing" title="CV" target="_blank"><i class="fa-solid fa-file fa-2xl"></i></a></div>
                <div><a href="https://github.com/TRS07170" title="GitHub" target="_blank"><i class="fa-brands fa-square-github fa-2xl"></i></a></div>
                <div><a href="https://twitter.com/traysen879" title="X" target="_blank"><i class="fa-brands fa-square-x-twitter fa-2xl"></i></a></div>
            </div>
        </div>

        <div id="motto">
            <!-- Young drem should not stop at heart, more to put into action. -->
            From small beginnning comes great things.
        </div>

        <div id="about-me">
            <h2>About me</h2>
            <p> I will start my Master's program at the <a href="https://cse.ucsd.edu/" target="_blank">University of California, San Diego</a> (UCSD) from September 2024. Before this, 
                I earned my Bachelor's degree at the <a href="https://cs.illinois.edu/" target="_blank">University of Illinois Urbana-Champaign</a> (UIUC). 
                My research interests lie mainly in Computer Vision (2D & 3D) and Machine Learning. I am also interested in exploring reinforcement learning.
                I aim to build intelligent tools and agents that can understand, enhance, and interact with the world around us. <span class="custom-br"></span>
                At UIUC, I have had the opportunity to work with <a href="https://saurabhg.web.illinois.edu/" target="_blank">Prof. Saurabh Gupta</a> and 
                <a href="https://ap229997.github.io/" target="_blank">Aditya Prakash</a> on 3D hand pose reconstruction. 
                I also met <a href="https://boranzhao.github.io/" target="_blank">Prof. Pan Zhao</a> here and collaborated with him on developing an agricultural RL Gymnasium. <span class="custom-br"></span>
                Before transferring to UIUC, I spent the first two years of my undergraduate studies at <a href="https://www.bucknell.edu/" target="_blank">Bucknell University</a> (BU), 
                where I had the chance to work with <a href="https://thiagoserra.com/" target="_blank">Prof. Thiago Serra</a> on Neural Network optimization.
            </p>
        </div>

        <div id="publications" >
            <h2>Publications <span>(*Equal contribution)</span></h2>
            <div id="pub-blocks">
                <div class="pub-block">
                    <div class="poster-box"><img src="./src/assets/posters/WildHands.png" alt=""></div>
                    <div class="poster-info">
                        <h4>3D Hand Pose Estimation in Everyday Egocentric Images</h4>
                        <p>Aditya Prakash, <span>Ruisen Tu</span>, Matthew Chang, Saurabh Gupta</p>
                        <div class="acceptance">
                            European Conference on Computer Vision <span class="conference">(ECCV)</span>, 2024
                        </div>
                        <div class="links">
                            <div class="abstract">[<a href="">Abstract</a>]</div>
                            <!-- <div class="paper">[<a href="https://arxiv.org/abs/2312.06583" target="_blank">Paper</a>]</div> -->
                            <div class="website">[<a href="https://ap229997.github.io/projects/hands/" target="_blank">Website</a>]</div>
                            <div class="bibtex">[<a href="">Bibtex</a>]</div>
                        </div>
                        <div class="modal-abstract modal" style="display: none;">
                            3D hand pose estimation in everyday egocentric images is challenging for several reasons: poor visual signal (occlusion from the object of interaction, low resolution & motion blur), large perspective distortion (hands are close to the camera), and lack of 3D annotations outside of controlled settings. While existing methods often use hand crops as input to focus on fine-grained visual information to deal with poor visual signal, the challenges arising from perspective distortion and lack of 3D annotations in the wild have not been systematically studied. We focus on this gap and explore the impact of different practices, i.e. crops as input, incorporating camera information, auxiliary supervision, scaling up datasets. We provide several insights that are applicable to both convolutional and transformer models leading to better performance. Based on our findings, we also present WildHands, a system for 3D hand pose estimation in everyday egocentric images. Zero-shot evaluation on 4 diverse datasets (H2O, AssemblyHands, Epic-Kitchens, Ego-Exo4D) demonstrate the effectiveness of our approach across 2D and 3D metrics, where we beat past methods by 7.4% - 66%. In system level comparisons, WildHands achieves the best 3D hand pose on ARCTIC egocentric split, outperforms FrankMocap across all metrics and HaMeR on 3 out of 6 metrics while being 10x smaller and trained on 5x less data.
                        </div>
                        <div class="modal-bibtex modal" style="display: none;">
                            @inproceedings{Prakash2024Hands,
                                <br>author = {Prakash, Aditya and Tu, Ruisen and Chang, Matthew and Gupta, Saurabh},
                                <br>title = {3D Hand Pose Estimation in Everyday Egocentric Images},
                                <br>booktitle = {European Conference on Computer Vision (ECCV)},
                                <br>year = {2024}
                                <br>}
                        </div>
                    </div>
                </div>

                <div class="pub-block">
                    <div class="poster-box"><img src="./src/assets/posters/3D_reconstruction_of_objects_in_hands_without_real_world_3D_supervision.jpg" alt=""></div>
                    <div class="poster-info">
                        <h4>3D Reconstruction of Objects in Hands without Real World 3D Supervision</h4>
                        <p>Aditya Prakash, Matthew Chang, Matthew Jin, <span>Ruisen Tu</span>, Saurabh Gupta</p>
                        <div class="acceptance">
                            European Conference on Computer Vision <span class="conference">(ECCV)</span>, 2024
                        </div>
                        <div class="links">
                            <div class="abstract">[<a href="">Abstract</a>]</div>
                            <!-- <div class="paper">[<a href="https://arxiv.org/abs/2312.06583" target="_blank">Paper</a>]</div> -->
                            <div class="website">[<a href="https://ap229997.github.io/projects/wild-hoi/" target="_blank">Website</a>]</div>
                            <div class="bibtex">[<a href="">Bibtex</a>]</div>
                        </div>
                        <div class="modal-abstract modal" style="display: none;">
                            Prior works for reconstructing hand-held objects from a single image train models on images paired with 3D shapes. Such data is challenging to gather in the real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of a) in-the-wild raw video data showing hand-object interactions and b) synthetic 3D shape collections. In this paper, we propose modules to leverage 3D supervision from these sources to scale up the learning of models for reconstructing hand-held objects. Specifically, we extract multiview 2D mask supervision from videos and 3D shape priors from shape collections. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. Our experiments in the challenging object generalization setting on in-the-wild MOW dataset show 11.6% relative improvement over models trained with 3D supervision on existing datasets.
                        </div>
                        <div class="modal-bibtex modal" style="display: none;">
                            @inproceedings{Prakash2024HOI,
                                <br>author = {Prakash, Aditya and Chang, Matthew and Jin, Matthew and Tu, Ruisen and Gupta, Saurabh},
                                <br>title = {3D Reconstruction of Objects in Hands without Real World 3D Supervision},
                                <br>booktitle = {European Conference on Computer Vision (ECCV)},
                                <br>year = {2024}
                                <br>}
                        </div>
                    </div>
                </div>
                
                <div class="pub-block">
                    <div class="poster-box"><img src="./src/assets/posters/getting_away.png" alt=""></div>
                    <div class="poster-info">
                        <h4>Getting Away with More Network Pruning: From Sparsity to Geometry and Linear Regions</h4>
                        <p>Junyang Cai*, Khai-Nguyen Nguyen*, Nishant Shrestha, Aidan Good, <span>Ruisen Tu</span>, Xin Yu, Shandian Zhe, Thiago Serra</p>
                        <div class="acceptance">
                            <div>International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research <span class="conference">(CPAIOR)</span>, 2023</div>
                            <!-- <p><span>ICLR</span> 2023 Workshop on Sparsity in Neural Networks</p> -->
                        </div>
                        <div class="links">
                            <div class="abstract">[<a href="">Abstract</a>]</div>
                            <div class="paper">[<a href="https://arxiv.org/abs/2301.07966" target="_blank">Paper</a>]</div>
                            <div class="bibtex">[<a href="">Bibtex</a>]</div>
                        </div>
                        <div class="modal-abstract modal" style="display: none;">
                            One surprising trait of neural networks is the extent to which their connections can be pruned with little to no effect on accuracy. But when we cross a critical level of parameter sparsity, pruning any further leads to a sudden drop in accuracy. This drop plausibly reflects a loss in model complexity, which we aim to avoid. In this work, we explore how sparsity also affects the geometry of the linear regions defined by a neural network, and consequently reduces the expected maximum number of linear regions based on the architecture. We observe that pruning affects accuracy similarly to how sparsity affects the number of linear regions and our proposed bound for the maximum number. Conversely, we find out that selecting the sparsity across layers to maximize our bound very often improves accuracy in comparison to pruning as much with the same sparsity in all layers, thereby providing us guidance on where to prune.
                        </div>
                        <div class="modal-bibtex modal" style="display: none;">
                            @misc{cai2023getting,
                                <br>title={Getting Away with More Network Pruning: From Sparsity to Geometry and Linear Regions},
                                <br>author={Junyang Cai and Khai-Nguyen Nguyen and Nishant Shrestha and Aidan Good and Ruisen Tu and Xin Yu and Shandian Zhe and Thiago Serra},
                                <br>year={2023},
                                <br>eprint={2301.07966},
                                <br>archivePrefix={arXiv},
                                <br>primaryClass={cs.LG}
                                <br>}
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div id="research">
            <h2>Research Experience</h2>
            <div id="res-blocks">
                <div class="res-block">
                    <div class="info">
                        <h3>Gupta AI Lab, University of Illinois Urbana-Champaign</h3>
                        <p>Research Assistant. May 2023-May 2024</p>
                        <p>Supervisor: <a href="https://saurabhg.web.illinois.edu/" target="_blank">Prof. Saurabh Gupta</a></p>
                        <p>Mentor: <a href="https://ap229997.github.io/" target="_blank">Aditya Prakash</a></p>
                    </div>
                    <img src="./src/assets/UIUC-badge.png" alt="UIUC Badge">
                </div>
                <div class="res-block">
                    <div class="info">
                        <h3>Autonomous Systems and Control Lab, University of Alabama</h3>
                        <p>Researcher (Remote). August 2023-June 2024</p>
                        <p>Supervisor: <a href="https://boranzhao.github.io/" target="_blank">Prof. Pan Zhao</a></p>
                    </div>
                    <img src="./src/assets/UA-badge.png" alt="Alabama Badge">
                </div>
                <div class="res-block">
                    <div class="info">
                        <h3>Advanced Analytics Research Lab, Bucknell University</h3>
                        <p>Research Assistant. February 2022-July 2022</p>
                        <p>Supervisor: <a href="https://thiagoserra.com/" target="_blank">Prof. Thiago Serra</a></p>
                    </div>
                    <img src="./src/assets/BU-badge.png" alt="Bucknell Badge">
                </div>
            </div>
        </div>
    </div>

    <script src="src/js/main.js"></script>
</body>
</html>